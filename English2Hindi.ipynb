{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English2Hindi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachitBansal/Transformer-NMT/blob/master/English2Hindi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ee6WrlFqni4",
        "colab_type": "code",
        "outputId": "5ea4bfac-92f4-4546-a285-496508234ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/parallel.tgz"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-07 17:20:50--  http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/parallel.tgz\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.130\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96098008 (92M) [application/x-gzip]\n",
            "Saving to: ‘parallel.tgz’\n",
            "\n",
            "parallel.tgz        100%[===================>]  91.65M  8.10MB/s    in 25s     \n",
            "\n",
            "2020-04-07 17:21:15 (3.66 MB/s) - ‘parallel.tgz’ saved [96098008/96098008]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAN3y52_q3xJ",
        "colab_type": "code",
        "outputId": "9269e8c8-716b-4d57-fd84-6964f57043a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!tar -xzvf parallel.tgz"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parallel/\n",
            "parallel/IITB.en-hi.hi\n",
            "parallel/IITB.en-hi.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNTZTolwV3JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "63cd1206-1e2b-4983-f03b-2500fd7dd691"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh7sC_tnW6Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cp parallel/IITB.en-hi.hi drive/My\\ Drive/IITB.en-hi.hi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVGYy-sVXIrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cp parallel/IITB.en-hi.en drive/My\\ Drive/IITB.en-hi.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFGGExecXPly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path1 = 'drive/My Drive/IITB.en-hi.hi'\n",
        "# path2 = 'drive/My Drive/IITB.en-hi.en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OTWY23wq80t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eng_sents = []\n",
        "# hi_sents =  []\n",
        "# i = 0\n",
        "# with open('parallel/IITB.en-hi.hi', 'rt') as f:\n",
        "#   while(i!=10**6):\n",
        "#     eng_sents.append('\\sep '+f.readline())\n",
        "#     i+=1\n",
        "\n",
        "# with open('parallel/IITB.en-hi.en', 'rt') as f:\n",
        "#   while(i!=2*10**6):\n",
        "#     hi_sents.append('\\sep '+f.readline())\n",
        "#     i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH5kYygbrKWc",
        "colab_type": "code",
        "outputId": "83292665-137a-46bf-995f-52ab6d7caa03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# print(len(eng_sents), len(hi_sents))\n",
        "# print(eng_sents[-1], hi_sents[-1])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000000 1000000\n",
            "\\sep गोदाम में रखे दृष्टिबंधक माल की संख्या बदलते रहने के कारण ऋणकर्ता अनिधिक कर्ज पर निर्भर होता है। \n",
            " \\sep Due to changing quantity of hypothicated items in godown, borrower has to depend on unfunded debt.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQ7U_alrgVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnVDD85Jw_sl",
        "colab_type": "code",
        "outputId": "ccad246a-e0ef-4570-ef37-b3fa907bf448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCouabMw8PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Q7_wlSxDaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokeniser_hindi = ByteLevelBPETokenizer()\n",
        "tokeniser_hindi.train(['parallel/IITB.en-hi.hi'], special_tokens= ['\\sep', '\\pad'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95wr7u0sxvsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokeniser_eng = ByteLevelBPETokenizer()\n",
        "tokeniser_eng.train(['parallel/IITB.en-hi.en'], special_tokens= ['\\sep', '\\pad'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX4W1O9Yyic8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTvbgT9W3zHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size_en = tokeniser_eng.get_vocab_size()\n",
        "vocab_size_hi = tokeniser_eng.get_vocab_size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvihOJ3Stt3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48342228-866a-4fc2-cef1-ae01c8362260"
      },
      "source": [
        "print(vocab_size_en, vocab_size_hi)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWht2FStrLDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model(nn.Module):\n",
        "  def __init__(self, s_lang = 'en', t_lang = 'hi'):\n",
        "    super(model, self).__init__()\n",
        "    if(s_lang == 'en'):\n",
        "      vocab_size_src = vocab_size_en\n",
        "      vocab_size_tgt = vocab_size_hi\n",
        "    elif(s_lang == 'hi'):\n",
        "      vocab_size_src = vocab_size_hi\n",
        "      vocab_size_tgt = vocab_size_en\n",
        "    self.emb_src = nn.Embedding(vocab_size_src, 512)\n",
        "    self.emb_tgt = nn.Embedding(vocab_size_tgt, 512)\n",
        "    enc_layer = nn.TransformerEncoderLayer(512, 8, 2048, 0.1, 'relu')\n",
        "    self.transf_enc = nn.TransformerEncoder(enc_layer, 6)\n",
        "    dec_layer = nn.TransformerDecoderLayer(512, 8, 2048, 0.1, 'relu')\n",
        "    self.transf_dec = nn.TransformerDecoder(dec_layer, 6)\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.linear = nn.Linear(512, vocab_size_tgt)\n",
        "\n",
        "  def get_memory_mask(self, max_len_src, max_len_tgt):\n",
        "    mask = torch.zeros(max_len_src, max_len_tgt)\n",
        "    for i in range(min(max_len_src-1, max_len_tgt-1)):\n",
        "      mask[i][i:] = -float('Inf')\n",
        "    return mask\n",
        "\n",
        "  def get_tgt_mask(self, max_len):\n",
        "    mask = torch.zeros(max_len, max_len)\n",
        "    for i in range(max_len-1):\n",
        "      mask[i][i+1:] = -float('Inf')\n",
        "    return mask\n",
        "\n",
        "  def forward(self, data_, i):\n",
        "    batch_x, batch_y = data_.getitem(i)\n",
        "    # print(batch_x.shape, batch_y.shape)\n",
        "    # batch_x = batch_x.transpose(0, 1)\n",
        "    # batch_y = batch_y.transpose(0, 1)\n",
        "    # mmask = self.get_memory_mask(batch_y.shape[1], batch_x.shape[1])\n",
        "    tmask = self.get_tgt_mask(batch_y.shape[1])\n",
        "    x = self.emb_src(batch_x)\n",
        "    y = self.emb_tgt(batch_y)\n",
        "    x = x.transpose(0,1)\n",
        "    y = y.transpose(0,1)\n",
        "    x = self.transf_enc(x)\n",
        "    # print(y.shape, x.shape, mmask.shape, tmask.shape)\n",
        "    x = self.transf_dec(tgt = y, memory = x, tgt_mask = tmask)\n",
        "    x = self.softmax(self.linear(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSkoMqcmucdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GEmZ10m2oj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence as padd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1R3HKKbu1RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class data(Dataset):\n",
        "  def __init__(self, data, batch_size = 32, max_len = 80, src_lang = 'en', tar_lang = 'hi'):\n",
        "    self.max_len = max_len\n",
        "    self.bsz = batch_size\n",
        "    self.data = data.values\n",
        "    self.src_lang = src_lang\n",
        "    self.tar_lang = tar_lang\n",
        "\n",
        "  def batchify(self, i):\n",
        "    batch_x  = []\n",
        "    batch_y = []\n",
        "    for j in range(self.bsz):\n",
        "      batch_x.append(self.data[i*self.bsz + j, 0])\n",
        "      batch_y.append(self.data[i*self.bsz + j, 1])\n",
        "\n",
        "    return batch_x, batch_y\n",
        "    \n",
        "  def getitem(self, i):\n",
        "    x, y = self.batchify(i)\n",
        "    return padd(x, True, tokeniser_eng.encode('\\pad').ids[0]), padd(y, True, tokeniser_eng.encode('\\pad').ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCqTsYiLBipa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhtDgv557o2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load():\n",
        "    i = 0\n",
        "    eng_sents = []\n",
        "    hi_sents =  []\n",
        "    i = 0\n",
        "    with open('./parallel/IITB.en-hi.en', 'rt') as f:\n",
        "      while(i!=10**6):\n",
        "        sent = tokeniser_eng.encode('\\sep '+f.readline()).ids\n",
        "        eng_sents.append(torch.tensor(sent))\n",
        "        i+=1\n",
        "\n",
        "    with open('./parallel/IITB.en-hi.hi', 'rt') as f:\n",
        "      while(i!=2*10**6):\n",
        "        hi_sents.append(torch.tensor(tokeniser_hindi.encode('\\sep '+f.readline()).ids))\n",
        "        i+=1\n",
        "\n",
        "    zipped_list = list(zip(eng_sents, hi_sents))\n",
        "    df_prllel_en_hi = pd.DataFrame(zipped_list, columns = ['en', 'hi'], dtype=object)\n",
        "    zipped_list = list(zip(hi_sents, eng_sents))\n",
        "    df_prllel_hi_en = pd.DataFrame(zipped_list, columns = ['hi', 'en'], dtype=object)\n",
        "    return df_prllel_en_hi, df_prllel_hi_en\n",
        "\n",
        "d_e_h, d_h_e = load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iEByMvu6XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def batchify(data, bsz = 32, src_lang = 'en', tar_lang = 'hi'):\n",
        "#   return {'x':data.loc[i*bsz:(i+1)*bsz-1, src_lang], 'y':self.data.loc[i*bsz:(i+1)*bsz-1, tar_lang]}\n",
        "\n",
        "# def batch_tokens(data, i, s_sz = 32, key = 'x'):\n",
        "#   return padd([data[i][key] for i in range(b_sz)], batch_first=True, padding_value=0)\n",
        "\n",
        "# def return_batch(data, b_sz, i):\n",
        "#   padd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWr86pon4RAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_lang = 'en'\n",
        "t_lang = 'hi'\n",
        "batch_size = 32\n",
        "batchs = d_e_h.shape[0]//batch_size\n",
        "data_ = data(d_e_h, batch_size, src_lang=s_lang, tar_lang=t_lang)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ESLKw0uvW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy(x, i, batch_first = False):\n",
        "  if(batch_first != True):\n",
        "    x = x.transpose(0, 1)\n",
        "  ans = torch.zeros(x.shape[0], x.shape[1])\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x[i].shape[0]):\n",
        "      ans[i][j] = torch.max(x[i][j], 0)[1]\n",
        "  print(ans.shape)\n",
        "  return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t98qg50DusbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aafc138c-8f7b-4549-87c9-358b90a645d9"
      },
      "source": [
        "batch_x, batch_y = data_.getitem(i)\n",
        "a = []\n",
        "for i in range(batch_x.shape[1]):\n",
        "  a.append(batch_x[21][i])\n",
        "\n",
        "print(tokeniser_eng.decode(a))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Browse the various methods of the current accessible\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JVD6cpWhl2dZ",
        "outputId": "52e89a71-cc50-4ade-ed9c-c35d79a97d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "model_ = model(s_lang, t_lang)\n",
        "optimizer = optim.Adam(model_.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for i in range(batchs):\n",
        "  batch_x, batch_y = data_.getitem(i)\n",
        "  x = batch_x\n",
        "  y = batch_y\n",
        "  # print(x)\n",
        "  x = x.transpose(0,1)\n",
        "  y = y.transpose(0,1)\n",
        "  # print(y.shape)\n",
        "  optimizer.zero_grad()\n",
        "  out = model_.forward(data_, i)\n",
        "  # print(out)\n",
        "  loss = criterion(out.reshape(-1, vocab_size_hi), y.reshape(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(i%20):\n",
        "    print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(10.3067, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3090, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3078, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3091, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3086, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3099, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3122, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3082, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3084, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3095, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3089, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3101, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3088, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3139, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3113, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3121, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3097, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3103, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3131, grad_fn=<NllLossBackward>)\n",
            "tensor(10.3095, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq5-dfgKTOjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}